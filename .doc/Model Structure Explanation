Inspired by the CNN model structure used here: https://github.com/IMFardz/AngryTops/tree/master/AngryTops

1. With convolution models it's common to begin and end on a dense layer because those layers are actually responsible for classification so you want them to see all of the features of the convolutional layers. This could end with just a single dense layer as well but decreasing complexity sometimes hurt performance.

2. The input shape of (8,8,4) has to match the size of the input layer which is 256. It's common to use binary-scale layer sizes and then scale up or down the size across similar layers.  The first layer for this problem will always have a significant number of trainable parameters because of the input size of 299*300 but the model training time can be sped up dramatically by reducing the size of the first dense layer and changing the shape of the layer so something like dense(8) -> reshape(2,2,2) or dense(16) -> reshape(4,2,2).

3. The convolution layers start small here with size 16 then scale up just because the first layer has so many trainable parameters so connecting those to a larger layer would slow down the model quite a bit. Then it scales up by factors of 2 (binary scaling common in ML). The (3,3) is a kernel size. This shape is another hyperparameter and it's basically saying that the layer will consider the image as combined sums of values from 3x3 pixel blocks. Smaller dimensions usually better with fine features but slows down training time.

4. Max pooling is very common with CNNs and accomplishes smoothing of training. What it's doing is reducing the model down to the most significant feature of a 2x2 pixel block then moving across 2 pixels at a time. This 2x2 with stride of 2 is the standard configuration for a max pooling layer used in most examples. The model might work best with batch norm (as in https://www.osapublishing.org/ol/abstract.cfm?uri=ol-44-21-5141), max pooling, both, or neither (this can be discovered through testing).

5. Lastly it flattens the shape to feed into dense to end. Again this is common. The activation for the terminal layer is linear instead of ReLU because it's a continuous output and not binary. ReLU is the norm for hidden layers because it seems to work well for deeply theoretical and not well understood reasons but only used in the terminal layer in binary classification.
