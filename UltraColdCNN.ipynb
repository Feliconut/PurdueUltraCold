{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.utils import shuffle\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training data and convert to tensorflow tensors\n",
    "resDir = r\"C:\\directory\\results\"\n",
    "trainin = pkl.load(open(resDir + \"\\\\trainin.pkl\", \"rb\"))\n",
    "trainout = pkl.load(open(resDir + \"\\\\trainout.pkl\", \"rb\"))\n",
    "testin = pkl.load(open(resDir + \"\\\\testin.pkl\", \"rb\"))\n",
    "testout = pkl.load(open(resDir + \"\\\\testout.pkl\", \"rb\"))\n",
    "yscaler = pkl.load(open(resDir + \"\\\\yscaler.pkl\", \"rb\"))\n",
    "trainin = tf.convert_to_tensor(trainin)\n",
    "trainout = tf.convert_to_tensor(trainout)\n",
    "testin = tf.convert_to_tensor(testin)\n",
    "testout = tf.convert_to_tensor(testout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true,y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true,y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input and output sizes\n",
    "input_size = tf.transpose(trainin).shape[0]\n",
    "output_size = tf.transpose(trainout).shape[0]\n",
    "\n",
    "#ML Hyperparameters\n",
    "lr = 10e-4 #learning rate\n",
    "epochs = 25 #number of passes of the data\n",
    "batchsize = 1 #number of images fed in at a time\n",
    "iterations = trainin.shape[0]//batchsize #number of batches used per epoch\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=.9, beta_2=.999, epsilon=1e-7, decay=0.) #ML optimizer\n",
    "loss_fn = mse #loss function\n",
    "\n",
    "metric =  keras.metrics.MeanSquaredError() #metric to be used for training\n",
    "val_metric = keras.metrics.MeanSquaredError() #metric to be used for validation\n",
    "\n",
    "#Configuration dictionary for ML model\n",
    "config = {'act1': 'relu', 'act2': 'linear', 'size1': 256, 'size2': 168, 'size3':28, 'size4': 7}\n",
    "\n",
    "#ML model in keras functional API (CNN with max pooling fed into dense with batch normalization) \n",
    "inputs=keras.Input(shape = (input_size,), name = 'input')\n",
    "x = Dense(int(config['size1']), input_shape = (input_size,), activation = config['act1'])(inputs)\n",
    "x = Reshape(target_shape = (8, 8, 4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = Conv2D(16, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = MaxPooling2D((2, 2), strides=2)(x)\n",
    "x = Conv2D(32, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = Conv2D(32, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = MaxPooling2D((2, 2), strides=2)(x)\n",
    "x = Conv2D(64, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = Conv2D(64, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = MaxPooling2D((2, 2), strides=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(int(config['size2']), activation = config['act1'])(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(int(config['size3']), activation = config['act1'])(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(int(config['size4']), activation = config['act1'])(x)\n",
    "x = BatchNormalization()(x)\n",
    "main_output = Dense(output_size, activation = config['act2'], name = 'main_output')(x)\n",
    "\n",
    "outputs = [main_output]\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "modelname = (\"BnCNN_%.1e_%f_%f\" % (lr, epochs, batchsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index list to be used for shuffling\n",
    "indices = tf.range(start=0, limit=tf.shape(trainin)[0], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that trains the model on a batch of declared size\n",
    "def train_model_on_batch():\n",
    "    #Use index list shuffling to create shuffled batches for training\n",
    "    start = epoch * batchsize\n",
    "    shuffled_indices = tf.random.shuffle(indices)\n",
    "    shuffledin = tf.gather(trainin, shuffled_indices)\n",
    "    shuffledout = tf.gather(trainout, shuffled_indices)\n",
    "    x_batch = shuffledin[start:start + batchsize, :]\n",
    "    y_batch = shuffledout[start:start + batchsize, :]\n",
    "    \n",
    "    #Gradient tape watches changes to loss as model is called\n",
    "    with tf.GradientTape() as tape:\n",
    "        if iteration % 1 == 0:\n",
    "            current_loss = loss_fn(y_batch, model(x_batch))\n",
    "    \n",
    "    #Apply gradients to optimizer based on shape of model\n",
    "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    \n",
    "    #Update the training metric\n",
    "    metric.update_state(y_batch, model(x_batch))\n",
    "    \n",
    "    \n",
    "    #Only calculate validation metric and validation loss once\n",
    "    if iteration == (iterations - 1):\n",
    "        val_metric.update_state(testout, model(testin))\n",
    "        val_loss = loss_fn(testout, model(testin))\n",
    "        return current_loss, metric.result(), val_loss, val_metric.result()\n",
    "    else:\n",
    "        return current_loss, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training history lists\n",
    "loss_history = []\n",
    "metric_history = []\n",
    "val_loss_history = []\n",
    "val_metric_history = []\n",
    "\n",
    "#Initial loss value below which the model will start saving\n",
    "min_loss = 1\n",
    "\n",
    "#Customizable model training loop\n",
    "for epoch in range(epochs):\n",
    "    for iteration in range(iterations):\n",
    "        current_loss, metric_result, val_loss, val_metric_result = train_model_on_batch()\n",
    "    #For runs with many epochs, control how often loss histories are recorded and printed\n",
    "    if epoch % 1 == 0:\n",
    "        loss_history.append(current_loss.numpy())\n",
    "        metric_history.append(metric_result.numpy())\n",
    "        val_loss_history.append(val_loss.numpy())\n",
    "        val_metric_history.append(val_metric_result.numpy())\n",
    "        print(\"\\nEpoch: {}/{} - Loss: {} - MSE: {} \\\\n\\nVal_Loss: {} - Val_MSE: {}\".format(\n",
    "            (epoch + 1), epochs, loss_history[-1], metric_history[-1], val_loss_history[-1], val_metric_history[-1]))\n",
    "    #Save the model as h5 filetype which is smaller than full model data\n",
    "    if val_metric_result < .3:\n",
    "        if val_metric_result < min_loss:\n",
    "            model.save(\"%s\\\\%s.h5\" % (resDir,modelname))\n",
    "            min_loss = val_metric_result\n",
    "    #Reset states of metrics\n",
    "    metric.reset_states()\n",
    "    val_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphic tool that stores a png summarizing model structure\n",
    "keras.utils.plot_model(model, '%s\\\\customlossmodel.png' % resDir, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An alternative way to print model structure\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot histograms comparing model and truth labels for parameters\n",
    "varnames = [\"A\", \"Tau\", \"S0\", \"Alpha\", \"Phi\", \"Beta\", \"Delta S\"]\n",
    "pred = tf.transpose(yscaler.inverse_transform(model(testin)))\n",
    "true = tf.transpose(yscaler.inverse_transform(testout))\n",
    "for i in range(pred.shape[0]):\n",
    "    plt.hist(pred[i], bins = 6, histtype='step', label = 'Model Pred')\n",
    "    plt.hist(true[i], bins = 6, histtype = 'step', label = 'Truth Label')\n",
    "    plt.title(varnames[i])\n",
    "    plt.legend()\n",
    "    plt.savefig(resDir + \"\\\\%s\" % varnames[i])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.numpy()-true.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(pred.numpy()-true.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
