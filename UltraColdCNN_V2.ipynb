{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from sklearn.utils import shuffle\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = './results/'\n",
    "trainin = pkl.load(open(savepath + \"trainin\", \"rb\"))\n",
    "trainout = pkl.load(open(savepath + \"trainout\", \"rb\"))\n",
    "testin = pkl.load(open(savepath + \"testin\", \"rb\"))\n",
    "testout = pkl.load(open(savepath + \"testout\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainin.shape)\n",
    "print(trainout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input and output sizes\n",
    "input_size = tf.transpose(trainin).shape[0]\n",
    "output_size = tf.transpose(trainout).shape[0]\n",
    "\n",
    "#ML Hyperparameters\n",
    "lr = 10e-4 #learning rate\n",
    "epochs = 100 #number of passes of the data\n",
    "batchsize = 256 #number of images fed in at a time\n",
    "iterations = trainin.shape[0]//batchsize #number of batches used per epoch\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=.9, beta_2=.999, epsilon=1e-4, decay=0.) #ML optimizer\n",
    "loss_fn = mae #loss function\n",
    "\n",
    "metric =  keras.metrics.MeanSquaredError() #metric to be used for training\n",
    "val_metric = keras.metrics.MeanSquaredError() #metric to be used for validation\n",
    "\n",
    "#Configuration dictionary for ML model\n",
    "config = {'act1': 'relu', 'act2': 'linear', 'size1': 256, 'size2': 48, 'size3':12, 'size4': 3}\n",
    "\n",
    "#ML model in keras functional API (CNN with max pooling fed into dense with batch normalization) \n",
    "inputs = keras.Input(shape = (input_size,), name = 'input')\n",
    "x = Dense(int(config['size1']), input_shape = (input_size,), activation = config['act1'])(inputs)\n",
    "x = Reshape(target_shape = (8, 8, 4))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(16, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = Conv2D(16, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = MaxPooling2D((2, 2), strides = 2)(x)\n",
    "x = Conv2D(32, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = Conv2D(32, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = MaxPooling2D((2, 2), strides = 2)(x)\n",
    "x = Conv2D(64, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = Conv2D(64, (3,3), activation = config['act1'], padding = \"same\")(x)\n",
    "x = MaxPooling2D((2, 2), strides = 2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(int(config['size2']), activation = config['act1'])(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(int(config['size3']), activation = config['act1'])(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(int(config['size4']), activation = config['act1'])(x)\n",
    "x = BatchNormalization()(x)\n",
    "main_output = Dense(output_size, activation = config['act2'], name = 'main_output')(x)\n",
    "\n",
    "outputs = [main_output]\n",
    "\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "modelname = (\"BnCNN_%.1e_%d_%d\" % (lr, epochs, batchsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index list to be used for shuffling\n",
    "indices = tf.range(start = 0, limit = tf.shape(trainin)[0], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function that trains the model on a batch of declared size\n",
    "# def train_model_on_batch():\n",
    "#     #Use index list shuffling to create shuffled batches for training\n",
    "#     start = iteration * batchsize\n",
    "#     x_batch = shuffledin[start:start + batchsize, :]\n",
    "#     y_batch = shuffledout[start:start + batchsize, :]\n",
    "    \n",
    "#     #Gradient tape watches changes to loss as model is called\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         current_loss = loss_fn(y_batch, model(x_batch))\n",
    "    \n",
    "#     #Apply gradients to optimizer based on shape of model\n",
    "#     gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "#     #Update the training metric\n",
    "#     metric.update_state(y_batch, model(x_batch))\n",
    "    \n",
    "    \n",
    "#     #Only calculate validation metric and validation loss once\n",
    "#     if iteration == (iterations - 1):\n",
    "#         val_metric.update_state(testout, model(testin))\n",
    "#         val_loss = loss_fn(testout, model(testin))\n",
    "#         return current_loss, metric.result(), val_loss, val_metric.result()\n",
    "#     else:\n",
    "#         return current_loss, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training history lists\n",
    "# loss_history = []\n",
    "# metric_history = []\n",
    "# val_loss_history = []\n",
    "# val_metric_history = []\n",
    "\n",
    "# #Initial loss value below which the model will start saving\n",
    "# min_loss = 1\n",
    "\n",
    "# #Customizable model training loop\n",
    "# for epoch in range(epochs):\n",
    "#     shuffled_indices = tf.random.shuffle(indices)\n",
    "#     shuffledin = tf.gather(trainin, shuffled_indices)\n",
    "#     shuffledout = tf.gather(trainout, shuffled_indices)\n",
    "#     for iteration in range(iterations):\n",
    "#         print(\"Iteration: %d / %d\" % (iteration, iterations))\n",
    "#         current_loss, metric_result, val_loss, val_metric_result = train_model_on_batch()\n",
    "#     #For runs with many epochs, control how often loss histories are recorded and printed\n",
    "#     if epoch % 1 == 0:\n",
    "#         loss_history.append(current_loss.numpy())\n",
    "#         metric_history.append(metric_result.numpy())\n",
    "#         val_loss_history.append(val_loss.numpy())\n",
    "#         val_metric_history.append(val_metric_result.numpy())\n",
    "#         print(\"\\nEpoch: {}/{} - Loss: {} - MSE: {} \\n\\nVal_Loss: {} - Val_MSE: {}\".format(\n",
    "#             (epoch + 1), epochs, loss_history[-1], metric_history[-1], val_loss_history[-1], val_metric_history[-1]))\n",
    "#     #Save the model as h5 filetype which is smaller than full model data\n",
    "#     if val_metric_result < .3:\n",
    "#         if val_metric_result < min_loss:\n",
    "#             model.save(\"%s\\\\%s.h5\" % (resDir,modelname))\n",
    "#             min_loss = val_metric_result\n",
    "#     #Reset states of metrics\n",
    "#     metric.reset_states()\n",
    "#     val_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizer, loss = 'mae', metrics = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainin, trainout, batch_size = batchsize, epochs = epochs, verbose = 2, validation_data = (testin, testout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./results/%s.h5\" % (modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
